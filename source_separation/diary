Doing experiments on the different AE, it look like there is a lot of randomness, that might be that sometimes we are lucky
with the init. Now I cut down what is going on to 3 options relative to 3 things I am doing:

Normal training = no minibatches, normal mean squared error,
With minibatches
With different normalization of the loss function

I think all this will effect also the VAE, I really want to see when the FUCK the AE is learning to map everything in
the same output, might be overwhelming weights

The outcome looks pretty random but can be cut down to 3 different cases
FIX, the reconstruction is the same for all possible inputs
CLUTTERED, reconstruction is kind of similar but is pretty quantized
DIE or MAGNIFICATION, the weights either die or make the rec to explode

In general the rec starts getting better after a while, I guess when the values stsrt making sense and they are not
close to zero.

Interesting fact: in the middle part it looks like sometimes the reconstruction is good but with the opposite sign,
should look into it

Something important is also to look why sometimes one neuron gets all the signal and the second one gets none

########## EXPERIMENTS ###########

Set up normal loss function but with batches of 64 with 15 epochs for layer 1 and 50 epochs for layer 2
around time 4 i get something like good reconstruction but a bit weak.
Time 6 still not bad in shape but weak
time 7 strangely the activity has exploded here,
and now it got kicked down
The problem is that when the rec is too weak I really loose a lot of signal especially if i need to reconstruct speech
Around time 13 the rec is not bad, 13 means 13*8 ms which are more or less 100 ms after the beginning that should mean
that there is something
Neuron 2 (green signal) is still pretty silent, but the reconstruction start being not bad, still not like this morning
though.
In all this it looks to me that the auto-encoder is still too slow to follow the signal.
It is slowly dying...

Stopped it to look at what is the rec in the internal layer, I put some more epochs (25-50)
Rec from first layer is good, this means that the first layer works fine, The second show exactly one of the problems
mentioned before, namely every input is mapped into the same reconstruction.
At time=4 I start seeing some arrangments, THIS is the layer that is slow in following! (ERROR propagation !?)
minibatches solved the problem for the first layer, I'll try to increase the number of minibatches for the layer 2.
Getting there.... at time 2 the rec is super good!
Stop and I will look at all the stages to see how much the single rec influences the complete pipeline
I loose a lot of energy in the second layer


[checking that the commands for complete and latent rec give the same res, yeah looks like]
With adam the first step is really good everything else is pretty slow

rec is good now but pretty at high energy...
but when I reconstruct my signal that is pretty low


------- My_VAE -------
I am trying different things with it, mainly I have the same problems as in the normal one, namely learning random bars
----------------------

PAUSING: cause nothing fucking works,

So the new idea is to use the transpose of the C_MAT to train, in this way the samples are far more and the dimensions
far less. It looks like it is working but I need to put a bottlenck of about 200 neurons to make is work.
The second problem is if to put the sample in or not, the answer is NO cause it fucks up a bi the scale of learning,
I want to see now what is the outcome on the reconstruction of the real sample.






---------------------------------------------------------------------
RECURRENT NEURAL NET
---------------------------------------------------------------------
First experiment with recurrent worked not bad, but of course I need to prove that is doing something intelligent and
not just learning by heart the senteces it needs to pull out.

- First Attempt: single_source_separation.py
    Various mixture of the same sentence with various other, The reconstruction works great even if it a bit artificial the voice
    at the output.
    Nevertheless I always have the same voice to reconstruct and this is too easy.
    In this case we have a Structure which is Encoder - Bottleneck - Decoder
    I would probably be able to do this without 3 layers but it is handy for afterwards

- Second attempt: single_noise_source_separation.py
    In this case I want to create a more complicated structure were the bottleneck connect to 2 different layers, namely
    the layers that needs to reconstruct the target and the layer that need to reconstruct the noise.
    Works ok for the desired source but distorts pretty badly the undesired, The idea is that the net is though to try
    to suppress always the same thing not no extract every time something different, that is a big bias that does
    not allow the net to learn to reconstruct different sources.

From here I will limit to 2 defined speakers, different sentences but always same speakers, in this way

??? HOW DO I USE THIS DATASET I JUST CREATED ????
SO What I have created is al the possible combinations of 332 sentences from a male and from a female, this give me
around... 110224 sample from which I wanna learn...

XXXXXXX forget it that is just too fucked up, it is less apinfull make up a matlab  function that creates the samples
for me on the fly

I set up a file "ss_2_matlab.py" to test this theory of having more mixed samples among which I train, at every epoch
15 MINIBATCHES of 12 samples each are created and the netowrk is trained, for now it looks kinda fucked up since the 
error should be balanced, ehile the error is higher in the 'noise' one, does matter. Keep 